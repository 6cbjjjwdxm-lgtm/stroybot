import os
from dotenv import load_dotenv 
load_dotenv() 
import pdfplumber
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω–¥–µ–∫—Å–æ–≤: { "–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏": FAISS_Index }
VECTOR_STORES = {}
EMBEDDINGS = OpenAIEmbeddings(model="text-embedding-3-small") # –î–µ—à–µ–≤–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å

def load_pdfs_from_folder(folder_path):
    """–ß–∏—Ç–∞–µ—Ç –≤—Å–µ PDF –≤ –ø–∞–ø–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    docs = []
    if not os.path.exists(folder_path):
        return []

    for filename in os.listdir(folder_path):
        if filename.lower().endswith('.pdf'):
            file_path = os.path.join(folder_path, filename)
            try:
                text = ""
                with pdfplumber.open(file_path) as pdf:
                    for page in pdf.pages:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç, —Å—Ç–∞—Ä–∞—è—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü
                        page_text = page.extract_text(layout=True)
                        if page_text:
                            text += page_text + "\n"
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –±–æ—Ç –∑–Ω–∞–ª, –æ—Ç–∫—É–¥–∞ –∏–Ω—Ñ–∞
                docs.append(Document(page_content=text, metadata={"source": filename}))
                logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {filename}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
    return docs

def build_index_for_project(project_name):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞"""
    base_folder = "StroyBot_Files"
    # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏ –∫–∞–∫ –≤ main.py
    project_clean = "".join([c if c.isalnum() or c in '._- ' else "_" for c in project_name]).strip()
    path = os.path.join(base_folder, project_clean)

    logger.info(f"üîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞: {project_name} ({path})")
    
    raw_docs = load_pdfs_from_folder(path)
    if not raw_docs:
        logger.warning(f"‚ö†Ô∏è –í –ø–∞–ø–∫–µ {path} –Ω–µ—Ç PDF —Ñ–∞–π–ª–æ–≤.")
        return None

    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∫—É—Å–æ—á–∫–∏ –ø–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã —É–¥–æ–±–Ω–æ —Å–∫–∞—Ä–º–ª–∏–≤–∞—Ç—å –ò–ò
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(raw_docs)

    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É (—ç—Ç–æ –∏ –µ—Å—Ç—å "–º–æ–∑–≥" –ø–æ–∏—Å–∫–∞)
    vectorstore = FAISS.from_documents(documents=splits, embedding=EMBEDDINGS)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å
    VECTOR_STORES[project_name] = vectorstore
    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å –¥–ª—è {project_name} –≥–æ—Ç–æ–≤! –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(splits)}")
    return vectorstore

def get_relevant_context(project_name, query):
    """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ—Ç - –ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å
    if project_name not in VECTOR_STORES:
        index = build_index_for_project(project_name)
        if not index:
            return None
    else:
        index = VECTOR_STORES[project_name]

    # –ò—â–µ–º 4 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞
    results = index.similarity_search(query, k=4)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
    context_text = "\n\n".join([f"--- –ò–ó –î–û–ö–£–ú–ï–ù–¢–ê: {doc.metadata['source']} ---\n{doc.page_content}" for doc in results])
    return context_text
